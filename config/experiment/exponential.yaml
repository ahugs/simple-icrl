# @package _global_
policy:
  lagrangian: 1 
  normalize_reward: False
reset_policy: True
reward:
  lr_scheduler: null
  optim:
    lr: 0.0001
  net:
    clip_range: null
    preprocess_net:
      norm_layer: null
    output_transform: 
        _target_: hydra.utils.get_method
        path: torch.exp
  # loss_transform:
  #     _target_: hydra.utils.get_method
  #     path: torch.log
    # clip_range: null
  bias_init: -4
  steps_per_epoch: 1
  regularization_coeff: 0.05
  regularization_type: "l1"