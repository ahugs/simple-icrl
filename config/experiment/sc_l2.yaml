# @package _global_
policy:
  lagrangian: 1 
  normalize_reward: False
reset_policy: False
reward:
  lr_scheduler: null
  net:
    initialize_zero: True
    clip_range: null
    preprocess_net:
      norm_layer: null
    output_activation: 
      _target_: hydra.utils.get_class
      path: torch.nn.LeakyReLU
  steps_per_epoch: 1
  regularization_coeff: 0.1
  regularization_type: "l2_expert"